{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\theom\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python311\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading environment llm_20_questions failed: Failed to import transformers.models.t5.modeling_t5 because of the following error (look up to see its traceback):\n",
      "No module named 'torch._custom_ops'\n"
     ]
    }
   ],
   "source": [
    "from utils.ActionGenerator import ActionsGenerator\n",
    "from utils.plot_regret import plot_regret \n",
    "from utils.Play import play \n",
    "from utils.Experiment import experiment\n",
    "from Agents.UCB import UCB\n",
    "from Agents.LinUCB import LinUCB\n",
    "from Agents.RB_agent import RegretBalancingAgent\n",
    "from Agents.RB_representation import RegretBalancing_Representation\n",
    "from environments.BanditEnv import BanditEnv\n",
    "from environments.BernoulliBanditEnv import BernoulliBanditEnv\n",
    "from environments.LinearBandit import LinearBandit\n",
    "from kaggle_environments import make, evaluate\n",
    "import gymnasium as gym\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "from Agents.EpsilonGreedy import EpsilonGreedy\n",
    "np.random.seed(1306)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Arms definition and Bernoulli environment\n",
    "n_arms = 2\n",
    "means = [0.2, 0.75]\n",
    "env = BernoulliBanditEnv(means = means)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Initialization of eps-algo, with fixed epsilon\n",
    "eps1 = EpsilonGreedy(K=len(means), FixedEpsilon = False, decay_param=10, m = 1) \n",
    "eps2 = EpsilonGreedy(K=len(means), FixedEpsilon = False, decay_param=25, m = 1) \n",
    "eps3 = EpsilonGreedy(K=len(means), FixedEpsilon = False, decay_param=40, m = 1) \n",
    "eps4 = EpsilonGreedy(K=len(means), FixedEpsilon = False, decay_param=55, m = 1) \n",
    "eps5 = EpsilonGreedy(K=len(means), FixedEpsilon = False, decay_param=70, m = 1) \n",
    "eps6 = EpsilonGreedy(K=len(means), FixedEpsilon = False, decay_param=75, m = 1) \n",
    "eps7 = EpsilonGreedy(K=len(means), FixedEpsilon = False, decay_param=90, m = 1) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def u_bound_exploration(t,d):\n",
    "    u = np.sqrt(t)\n",
    "    return u "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "rb = RegretBalancing_Representation([eps1, eps2, eps3, eps4, eps5, eps6, eps7], K=n_arms, u_bound=u_bound_exploration, env=env)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "epsOpti = EpsilonGreedy(K=len(means), Delta = 0.55, m = 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "RegretBalancing_Representation.get_action() takes 2 positional arguments but 3 were given",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[7], line 4\u001b[0m\n\u001b[0;32m      2\u001b[0m Nmc \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m100\u001b[39m  \u001b[38;5;66;03m# Number of Monte Carlo simulations\u001b[39;00m\n\u001b[0;32m      3\u001b[0m T \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m2000\u001b[39m  \u001b[38;5;66;03m# Number of rounds\u001b[39;00m\n\u001b[1;32m----> 4\u001b[0m all_data \u001b[38;5;241m=\u001b[39m \u001b[43mexperiment\u001b[49m\u001b[43m(\u001b[49m\u001b[43menv\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m[\u001b[49m\u001b[43mrb\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mepsOpti\u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mNmc\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mT\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\theom\\Desktop\\AML\\Advanced-ML\\src\\utils\\Experiment.py:13\u001b[0m, in \u001b[0;36mexperiment\u001b[1;34m(environment, agents, Nmc, T)\u001b[0m\n\u001b[0;32m     11\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m agent \u001b[38;5;129;01min\u001b[39;00m agents:\n\u001b[0;32m     12\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m agent\u001b[38;5;241m.\u001b[39mname() \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mRB\u001b[39m\u001b[38;5;124m'\u001b[39m:\n\u001b[1;32m---> 13\u001b[0m         agent_id, regrets \u001b[38;5;241m=\u001b[39m \u001b[43mplay_RB\u001b[49m\u001b[43m(\u001b[49m\u001b[43menvironment\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43magent\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mNmc\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mT\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     14\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m     15\u001b[0m         agent_id, regrets \u001b[38;5;241m=\u001b[39m play(environment, agent, Nmc, T)\n",
      "File \u001b[1;32mc:\\Users\\theom\\Desktop\\AML\\Advanced-ML\\src\\utils\\play_RB.py:17\u001b[0m, in \u001b[0;36mplay_RB\u001b[1;34m(environment, agent, Nmc, T)\u001b[0m\n\u001b[0;32m     15\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m t \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(T):\n\u001b[0;32m     16\u001b[0m     action_set \u001b[38;5;241m=\u001b[39m environment\u001b[38;5;241m.\u001b[39mget_action_set()\n\u001b[1;32m---> 17\u001b[0m     action \u001b[38;5;241m=\u001b[39m \u001b[43magent\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget_action\u001b[49m\u001b[43m(\u001b[49m\u001b[43maction_set\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mt\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     18\u001b[0m     action, learner \u001b[38;5;241m=\u001b[39m agent\u001b[38;5;241m.\u001b[39mget_action(action_set)\n\u001b[0;32m     19\u001b[0m     reward \u001b[38;5;241m=\u001b[39m environment\u001b[38;5;241m.\u001b[39mget_reward(action)\n",
      "\u001b[1;31mTypeError\u001b[0m: RegretBalancing_Representation.get_action() takes 2 positional arguments but 3 were given"
     ]
    }
   ],
   "source": [
    "#Run experiments\n",
    "Nmc = 100  # Number of Monte Carlo simulations\n",
    "T = 2000  # Number of rounds\n",
    "all_data = experiment(env, [rb, epsOpti], Nmc, T)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Update rcParams\n",
    "plt.rcParams.update({\n",
    "    'axes.titlesize': 16,           # Title size for the axes\n",
    "    'axes.labelsize': 16,           # Label size for the axes\n",
    "    'xtick.labelsize': 14,          # Label size for x-axis ticks\n",
    "    'ytick.labelsize': 14,          # Label size for y-axis ticks\n",
    "    'legend.fontsize': 14,          # Font size for legend\n",
    "    'text.usetex': True,            # Use LaTeX for text rendering\n",
    "    'font.family': 'serif',         # Use serif font (LaTeX default)\n",
    "})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "#Plot results\n",
    "plot_regret(regrets = all_data, logscale = False, lb = None, q = 10, save_pdf='decayEps.pdf')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
